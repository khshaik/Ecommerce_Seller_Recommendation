# Quick Reference Guide

## ğŸš€ Quick Start (5 Minutes)

### Prerequisites Check
```bash
python3 --version          # Python 3.8+
java -version              # Java 11+
spark-submit --version     # Spark 3.5.2
aws s3 ls                  # AWS credentials configured
```

### Setup
```bash
cd DSP_GA_2025em1100102_20112025/
python3 -m venv .venv
source .venv/bin/activate
pip install pyspark==3.5.2 delta-spark==3.2.0 pyarrow pyyaml s3fs pandas
```

### Run Pipeline
```bash
# Company Sales ETL
./2025em1100102/ecommerce_seller_recommendation/s3/scripts/etl_company_sales_spark_submit.sh

# Competitor Sales ETL
./2025em1100102/ecommerce_seller_recommendation/s3/scripts/etl_competitor_sales_spark_submit.sh

# Seller Catalog ETL
./2025em1100102/ecommerce_seller_recommendation/s3/scripts/etl_seller_catalog_spark_submit.sh

# Generate Recommendations
./2025em1100102/ecommerce_seller_recommendation/s3/scripts/consumption_recommendation_spark_submit.sh
```

### Verify Output
```bash
aws s3 ls s3://2025em1100102/dsp_ga_2025em1100102_20112025/processed/recommendations/
```

---

## ğŸ“ Key File Locations

| File | Purpose | Location |
|------|---------|----------|
| **Configuration** | Production config | `s3/configs/ecomm_prod.yml` |
| **Company Sales ETL** | Process company data | `s3/src/etl_company_sales.py` |
| **Competitor Sales ETL** | Process competitor data | `s3/src/etl_competitor_sales.py` |
| **Seller Catalog ETL** | Process catalog data | `s3/src/etl_seller_catalog.py` |
| **Recommendations** | Generate recommendations | `s3/src/consumption_recommendation.py` |
| **DQ Functions** | Data quality utilities | `s3/libs/dq.py` |
| **Spark Setup** | Spark initialization | `s3/libs/spark_session.py` |
| **Tests** | Validation scripts | `s3/tests/` |

---

## ğŸ”„ Data Flow

```
Raw CSV
  â†“
Bronze (Snapshot)
  â†“
DQ Validation
  â†“
Silver (Clean) + Quarantine (Invalid)
  â†“
Gold (Aggregates)
  â†“
Recommendations
```

---

## ğŸ“Š Data Schemas

### Company Sales Input
```
item_id (string)
seller_id (string)
units_sold (int)
revenue (double)
marketplace_price (double)
sale_date (string)
```

### Recommendations Output
```
seller_id (string)
item_id (string)
item_name (string)
category (string)
market_price (double)
expected_units_sold (double)
expected_revenue (double)
```

---

## âš™ï¸ Configuration Quick Edit

Edit `s3/configs/ecomm_prod.yml`:

```yaml
# Update S3 bucket name
paths:
  input_root: "s3a://YOUR-BUCKET/dsp_ga_2025em1100102_20112025/input/raw"
  output_root: "s3a://YOUR-BUCKET/dsp_ga_2025em1100102_20112025/output"

# Adjust for your cluster
spark:
  shuffle_partitions: 8  # Increase for larger clusters
```

---

## ğŸ§ª Run Tests

```bash
# Test recommendations
python 2025em1100102/ecommerce_seller_recommendation/s3/tests/test_consumption_recommendations.py

# Validate logic
python 2025em1100102/ecommerce_seller_recommendation/s3/tests/validate_logic.py

# Check expected units
python 2025em1100102/ecommerce_seller_recommendation/s3/tests/validate_expected_units.py
```

---

## ğŸ“ˆ Key Metrics

| Metric | Expected | Location |
|--------|----------|----------|
| Total Bronze Records | ~1,000,000 | `output/bronze/` |
| Valid Silver Records | ~903,696 | `output/silver/` |
| Quarantine Records | ~96,304 | `output/quarantine/` |
| Recommendations | Per seller | `processed/recommendations/` |

---

## ğŸ” DQ Rules Summary

### Company Sales
- âœ… item_id, seller_id NOT NULL
- âœ… units_sold, revenue, marketplace_price â‰¥ 0
- âœ… sale_date â‰¤ today
- âœ… Deduplicate by (seller_id, item_id)

### Competitor Sales
- âœ… order_id, customer_id, product_id NOT NULL
- âœ… qty, unit_price â‰¥ 0
- âœ… order_ts valid timestamp

### Seller Catalog
- âœ… seller_id, item_id NOT NULL
- âœ… marketplace_price, stock_qty â‰¥ 0

---

## ğŸ› Common Issues

| Issue | Solution |
|-------|----------|
| S3 timeout | Increase `fs.s3a.connection.timeout` in config |
| Out of memory | Reduce `shuffle_partitions` or increase executor memory |
| Hudi table not found | Verify S3 paths in config match actual structure |
| DQ failures | Check `output/quarantine/` for invalid records |
| AWS credentials error | Run `aws configure` or set env vars |

---

## ğŸ“š Documentation Files

| File | Content |
|------|---------|
| `PROJECT_OVERVIEW.md` | Architecture, workflow, quick start |
| `SETUP_GUIDE.md` | Installation, configuration, troubleshooting |
| `TECHNICAL_GUIDE.md` | Implementation details, code examples |
| `CODEBASE_STRUCTURE.md` | Module reference, file organization |
| `SEQUENCE_DIAGRAM.md` | ETL sequence flow |
| `ARCHITECTURE_WORKFLOW.svg` | Visual architecture diagram |

---

## ğŸ”— Useful Commands

```bash
# List S3 contents
aws s3 ls s3://bucket-name/ --recursive

# Copy from S3
aws s3 cp s3://bucket/path/file.csv ./local-file.csv

# Monitor Spark job
# Open http://localhost:4040 while job running

# Check Spark logs
tail -f /path/to/spark/logs/spark-*.log

# Kill Spark job
pkill -f spark-submit

# Test S3 access
python -c "import boto3; s3=boto3.client('s3'); print(s3.list_buckets())"
```

---

## ğŸ“ Getting Help

1. **Check logs:** `helper/logs/clean/etl/logs.txt`
2. **Review DQ failures:** `output/quarantine/`
3. **Validate input:** Check CSV format matches schema
4. **Test S3 access:** `aws s3 ls s3://bucket-name/`
5. **Check config:** Verify paths in `ecomm_prod.yml`
6. **Review tests:** Run validation scripts in `tests/`

---

## ğŸ¯ Next Steps After Setup

1. âœ… Verify all prerequisites installed
2. âœ… Configure AWS credentials
3. âœ… Update `ecomm_prod.yml` with your S3 paths
4. âœ… Run company sales ETL
5. âœ… Check output in S3
6. âœ… Run all ETL scripts
7. âœ… Generate recommendations
8. âœ… Validate output CSVs
9. âœ… Run test scripts
10. âœ… Review metrics and logs

---

## ğŸ’¡ Pro Tips

- **Local Development:** Use `ecomm_local.yml` with local file paths
- **Debugging:** Set `log_level: "INFO"` in config for detailed logs
- **Performance:** Increase `shuffle_partitions` for larger datasets
- **Monitoring:** Check Spark UI at `http://localhost:4040`
- **Testing:** Run validation scripts before production runs
- **Backups:** Keep quarantine files for audit trails

---

## ğŸ”„ Typical Workflow

```
1. Setup Environment
   â””â”€ Install dependencies, configure AWS

2. Configure Project
   â””â”€ Update ecomm_prod.yml with S3 paths

3. Run ETL Pipeline
   â””â”€ Execute company, competitor, catalog ETLs

4. Monitor Execution
   â””â”€ Check logs, verify output in S3

5. Generate Recommendations
   â””â”€ Run consumption_recommendation.py

6. Validate Results
   â””â”€ Run test scripts, review metrics

7. Deploy/Consume
   â””â”€ Use recommendation CSVs in downstream systems
```

---

## ğŸ“Š Output Directory Structure

```
s3://bucket/dsp_ga_2025em1100102_20112025/
â”œâ”€â”€ input/raw/
â”‚   â”œâ”€â”€ company_sales/
â”‚   â”œâ”€â”€ competitor_sales/
â”‚   â””â”€â”€ seller_catalog/
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ company_sales/run_date=*/
â”‚   â”‚   â”œâ”€â”€ competitor_sales/run_date=*/
â”‚   â”‚   â””â”€â”€ seller_catalog/run_date=*/
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ company_sales/
â”‚   â”‚   â”œâ”€â”€ competitor_sales/
â”‚   â”‚   â””â”€â”€ seller_catalog/
â”‚   â””â”€â”€ quarantine/
â”‚       â”œâ”€â”€ company_sales/run_date=*/
â”‚       â”œâ”€â”€ competitor_sales/run_date=*/
â”‚       â””â”€â”€ seller_catalog/run_date=*/
â””â”€â”€ processed/
    â”œâ”€â”€ gold/
    â”‚   â”œâ”€â”€ company_sales_hudi/
    â”‚   â”œâ”€â”€ competitor_sales_hudi/
    â”‚   â””â”€â”€ seller_catalog_hudi/
    â””â”€â”€ recommendations/
        â”œâ”€â”€ company/
        â”‚   â””â”€â”€ company_seller_recommendation.csv
        â””â”€â”€ competitor/
            â””â”€â”€ competitor_seller_recommendation.csv
```

---

## ğŸ“ Learning Path

**Beginner:**
1. Read PROJECT_OVERVIEW.md
2. Follow SETUP_GUIDE.md
3. Run first ETL script
4. Check outputs in S3

**Intermediate:**
1. Review TECHNICAL_GUIDE.md
2. Study ETL scripts
3. Understand DQ rules
4. Run test scripts

**Advanced:**
1. Study CODEBASE_STRUCTURE.md
2. Modify DQ rules
3. Optimize performance
4. Extend functionality

---

## ğŸ“ Version Info

- **Project ID:** 2025em1100102
- **Spark Version:** 3.5.2
- **Hudi Version:** 0.15.0
- **Python:** 3.8+
- **Last Updated:** November 2025

---

## ğŸ”— External Resources

- [Apache Spark Docs](https://spark.apache.org/docs/latest/)
- [Apache Hudi Docs](https://hudi.apache.org/)
- [AWS S3 Guide](https://docs.aws.amazon.com/s3/)
- [Medallion Architecture](https://www.databricks.com/blog/2022/06/24/use-the-medallion-architecture-to-build-data-pipelines.html)

---

**Quick Links:**
- ğŸ“– Full Documentation: See PROJECT_OVERVIEW.md
- ğŸ”§ Setup Instructions: See SETUP_GUIDE.md
- ğŸ’» Code Reference: See TECHNICAL_GUIDE.md
- ğŸ“‚ File Organization: See CODEBASE_STRUCTURE.md
