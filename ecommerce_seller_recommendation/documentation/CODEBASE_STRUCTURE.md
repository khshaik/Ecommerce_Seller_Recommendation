# Codebase Structure & Module Reference

## ðŸ“‚ Directory Organization

```
ecommerce_seller_recommendation/
â”‚
â”œâ”€â”€ s3/                                    # Main S3-based production pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ conf/
â”‚   â”‚   â””â”€â”€ hudi-defaults.conf            # Hudi table configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ configs/                          # Configuration files
â”‚   â”‚   â”œâ”€â”€ ecomm_prod.yml                # Production config (S3 paths)
â”‚   â”‚   â”œâ”€â”€ ecomm_local.yml               # Local development config
â”‚   â”‚   â””â”€â”€ ecommprod.yml                 # Alternative config
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                             # Sample data & outputs
â”‚   â”‚   â”œâ”€â”€ dqcheck/                      # Sample dirty data for testing
â”‚   â”‚   â”‚   â”œâ”€â”€ company_sales_dirty.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ competitor_sales_dirty.csv
â”‚   â”‚   â”‚   â””â”€â”€ seller_catalog_dirty.csv
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ local/                        # Local execution outputs
â”‚   â”‚       â”œâ”€â”€ clean/                    # Clean data processing results
â”‚   â”‚       â”‚   â”œâ”€â”€ input/raw/            # Input CSV files
â”‚   â”‚       â”‚   â”œâ”€â”€ output/               # Bronze, Silver, Quarantine
â”‚   â”‚       â”‚   â””â”€â”€ processed/            # Gold & Recommendations
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ dirty/                    # Dirty data processing results
â”‚   â”‚           â”œâ”€â”€ input/raw/
â”‚   â”‚           â”œâ”€â”€ output/
â”‚   â”‚           â””â”€â”€ processed/
â”‚   â”‚
â”‚   â”œâ”€â”€ libs/                             # Reusable utility modules
â”‚   â”‚   â”œâ”€â”€ spark_session.py              # Spark initialization & config
â”‚   â”‚   â”œâ”€â”€ dq.py                         # Data quality validation functions
â”‚   â”‚   â”œâ”€â”€ log.py                        # Logging utilities
â”‚   â”‚   â””â”€â”€ utils.py                      # General utility functions
â”‚   â”‚
â”‚   â”œâ”€â”€ src/                              # Main ETL & analytics scripts
â”‚   â”‚   â”œâ”€â”€ etl_company_sales.py          # Company sales ETL pipeline
â”‚   â”‚   â”œâ”€â”€ etl_competitor_sales.py       # Competitor sales ETL pipeline
â”‚   â”‚   â”œâ”€â”€ etl_seller_catalog.py         # Seller catalog ETL pipeline
â”‚   â”‚   â”œâ”€â”€ consumption_recommendation.py # Recommendation engine
â”‚   â”‚   â”œâ”€â”€ analysis/                     # Analysis & exploration scripts
â”‚   â”‚   â””â”€â”€ read_file.py                  # File reading utility
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                          # Bash orchestration scripts
â”‚   â”‚   â”œâ”€â”€ etl_company_sales_spark_submit.sh
â”‚   â”‚   â”œâ”€â”€ etl_competitor_sales_spark_submit.sh
â”‚   â”‚   â”œâ”€â”€ etl_seller_catalog_spark_submit.sh
â”‚   â”‚   â”œâ”€â”€ consumption_recommendation_spark_submit.sh
â”‚   â”‚   â””â”€â”€ analysis/                     # Category-specific scripts
â”‚   â”‚       â”œâ”€â”€ category/
â”‚   â”‚       â””â”€â”€ noncategory/
â”‚   â”‚
â”‚   â”œâ”€â”€ helper/                           # Helper files & logs
â”‚   â”‚   â”œâ”€â”€ dqcheck/                      # DQ validation results
â”‚   â”‚   â””â”€â”€ logs/                         # Execution logs
â”‚   â”‚       â”œâ”€â”€ clean/
â”‚   â”‚       â”‚   â”œâ”€â”€ etl/logs.txt
â”‚   â”‚       â”‚   â””â”€â”€ recommendations/logs.txt
â”‚   â”‚       â””â”€â”€ dirty/
â”‚   â”‚           â”œâ”€â”€ etl/logs.txt
â”‚   â”‚           â””â”€â”€ recommendations/logs.txt
â”‚   â”‚
â”‚   â””â”€â”€ tests/                            # Test & validation scripts
â”‚       â”œâ”€â”€ test_consumption_recommendations.py
â”‚       â”œâ”€â”€ validate_expected_units.py
â”‚       â”œâ”€â”€ validate_logic.py
â”‚       â””â”€â”€ analyze_already_owned.py
â”‚
â””â”€â”€ local/                                # Local development mirror
    â”œâ”€â”€ config/
    â”œâ”€â”€ src/
    â”œâ”€â”€ libs/
    â””â”€â”€ scripts/
```

---

## ðŸ”§ Core Modules

### 1. Spark Session Module (`libs/spark_session.py`)

**Purpose:** Initialize and configure SparkSession with all required dependencies

**Key Functions:**

```python
def get_spark(app_name="ETL") -> Tuple[SparkSession, Dict]:
    """
    Initialize SparkSession with Hudi + S3 + Delta support
    
    Returns:
        Tuple[SparkSession, Dict]: Spark session and loaded config
    """
```

**Configuration Includes:**
- JAR files for Hadoop, AWS, Hudi, Delta
- S3 filesystem configuration
- Kryo serialization for Hudi
- Connection timeout & retry settings
- Adaptive query execution

**Usage:**
```python
from libs.spark_session import get_spark
spark, config = get_spark("MyETLJob")
```

---

### 2. Data Quality Module (`libs/dq.py`)

**Purpose:** Provide reusable DQ validation functions

**Functions:**

| Function | Purpose | Example |
|----------|---------|---------|
| `enforce_not_null()` | Remove null values | `enforce_not_null(df, ['id', 'name'])` |
| `enforce_positive()` | Validate min values | `enforce_positive(df, {'price': 0})` |
| `enforce_max()` | Validate max values | `enforce_max(df, {'qty': 10000})` |
| `enforce_regex()` | Pattern matching | `enforce_regex(df, 'email', r'.*@.*')` |
| `enforce_allowed_values()` | Enum validation | `enforce_allowed_values(df, 'status', ['ACTIVE', 'INACTIVE'])` |
| `dedupe_by_latest()` | Remove duplicates | `dedupe_by_latest(df, ['id'], 'timestamp')` |

**Usage:**
```python
from libs.dq import enforce_not_null, enforce_positive

df = spark.read.csv("data.csv", header=True)
df = enforce_not_null(df, ['item_id', 'seller_id'])
df = enforce_positive(df, {'units_sold': 0, 'revenue': 0})
```

---

### 3. Logging Module (`libs/log.py`)

**Purpose:** Centralized logging configuration

**Functions:**

```python
def get_logger(name: str) -> Logger:
    """Get configured logger instance"""
```

**Usage:**
```python
from libs.log import get_logger

logger = get_logger(__name__)
logger.info("Processing started")
logger.warning("High quarantine rate")
logger.error("Failed to write output")
```

---

### 4. Utilities Module (`libs/utils.py`)

**Purpose:** General utility functions

**Common Functions:**
- Path manipulation
- Date/time utilities
- String normalization
- Configuration helpers

---

## ðŸ“Š ETL Scripts

### Company Sales ETL (`src/etl_company_sales.py`)

**Responsibility:** Process company's own sales transactions

**Input:**
- CSV file: `company_sales_dirty.csv`
- Columns: item_id, seller_id, units_sold, revenue, marketplace_price, sale_date

**Processing:**
1. Read raw CSV
2. Write to Bronze (immutable snapshot)
3. Apply DQ rules
4. Deduplicate by (seller_id, item_id)
5. Split valid/invalid records
6. Write Silver (clean data)
7. Write Quarantine (failed records)

**Output:**
- Bronze: `output/bronze/company_sales/run_date=*/`
- Silver: `output/silver/company_sales/`
- Quarantine: `output/quarantine/company_sales/run_date=*/`

**DQ Rules:**
```yaml
Required fields: item_id, seller_id, units_sold, revenue, sale_date
Numeric checks: units_sold >= 0, revenue >= 0, marketplace_price >= 0
Date checks: sale_date <= today, multiple formats supported
Deduplication: Keep first valid (seller_id, item_id) combination
```

**Execution:**
```bash
spark-submit \
  --packages org.apache.hudi:hudi-spark3.5-bundle_2.12:0.15.0 \
  src/etl_company_sales.py \
  --config configs/ecomm_prod.yml
```

---

### Competitor Sales ETL (`src/etl_competitor_sales.py`)

**Responsibility:** Process competitor market data

**Input:**
- CSV file: `competitor_sales_dirty.csv`
- Columns: order_id, customer_id, product_id, order_ts, qty, unit_price

**Processing:**
Similar to company sales but with:
- Different column names (product_id vs item_id)
- Timestamp instead of date
- Customer-level analysis support

**Output:**
- Bronze, Silver, Quarantine (same structure as company sales)

---

### Seller Catalog ETL (`src/etl_seller_catalog.py`)

**Responsibility:** Process seller product catalogs

**Input:**
- CSV file: `seller_catalog_dirty.csv`
- Columns: seller_id, item_id, item_name, category, marketplace_price, stock_qty

**Processing:**
1. Normalize item IDs
2. Validate seller-item combinations
3. Extract category features
4. Validate stock levels
5. Apply DQ rules

**Output:**
- Bronze, Silver, Quarantine (same structure)

---

### Recommendation Engine (`src/consumption_recommendation.py`)

**Responsibility:** Generate item recommendations for sellers

**Algorithm:**

```
1. Load Hudi tables (company, competitor, catalog)
2. Aggregate sales by category & item
3. Identify top-N items per category
4. Create seller Ã— item combinations (cross join)
5. Remove already-owned items (left anti join)
6. Calculate expected units & revenue
7. Write recommendations to CSV
```

**Input:**
- Gold tables: company_sales_hudi, competitor_sales_hudi, seller_catalog_hudi

**Output:**
- `processed/recommendations/company/company_seller_recommendation.csv`
- `processed/recommendations/competitor/competitor_seller_recommendation.csv`

**Output Schema:**
```
seller_id, item_id, item_name, category, market_price, 
expected_units_sold, expected_revenue
```

**Execution:**
```bash
spark-submit \
  --packages org.apache.hudi:hudi-spark3.5-bundle_2.12:0.15.0 \
  src/consumption_recommendation.py \
  --config configs/ecomm_prod.yml
```

---

## ðŸ§ª Test Scripts

### Test Consumption Recommendations (`tests/test_consumption_recommendations.py`)

**Purpose:** Validate recommendation generation logic

**Tests:**
- Recommendation count per seller
- Expected units calculation
- Revenue estimation accuracy
- Missing item identification

---

### Validate Expected Units (`tests/validate_expected_units.py`)

**Purpose:** Verify expected units calculation

**Formula:**
```
expected_units = total_units_sold / number_of_sellers_selling_item
```

**Validation:**
- Check calculation accuracy
- Verify no division by zero
- Validate against historical data

---

### Validate Logic (`tests/validate_logic.py`)

**Purpose:** Verify business logic correctness

**Checks:**
- Top-N selection per category
- Duplicate removal
- Revenue calculations
- Partition correctness

---

### Analyze Already Owned (`tests/analyze_already_owned.py`)

**Purpose:** Analyze seller ownership patterns

**Analysis:**
- Items per seller
- Category distribution
- Overlap between sellers
- Recommendation coverage

---

## âš™ï¸ Configuration Files

### Production Config (`configs/ecomm_prod.yml`)

**Sections:**

1. **Spark Configuration**
   ```yaml
   spark:
     shuffle_partitions: 8
     log_level: "WARN"
     configs:
       # S3, Delta, Hudi configs
   ```

2. **Data Paths**
   ```yaml
   paths:
     input_root: "s3a://bucket/input/raw"
     output_root: "s3a://bucket/output"
     bronze_*: "s3a://bucket/output/bronze/*"
     silver_*: "s3a://bucket/output/silver/*"
     gold_*: "s3a://bucket/processed/gold/*"
   ```

3. **Data Quality Rules**
   ```yaml
   dq:
     company_sales_required: [item_id, seller_id, ...]
     company_sales_numeric_checks: {...}
     company_sales_date_checks: {...}
   ```

4. **Hudi Configuration**
   ```yaml
   tables:
     company_sales_hudi_table: "company_sales_data"
     competitor_sales_hudi_table: "competitor_sales_data"
     seller_catalog_hudi_table: "seller_catalog_data"
   ```

---

### Hudi Configuration (`conf/hudi-defaults.conf`)

**Key Settings:**
- Table type (COW/MOR)
- Record key field
- Partition path field
- Precombine field
- Compaction settings

---

## ðŸš€ Orchestration Scripts

### Company Sales Script (`scripts/etl_company_sales_spark_submit.sh`)

**Purpose:** Wrapper script for company sales ETL

**Responsibilities:**
- Set environment variables
- Prepare Spark submit arguments
- Load JAR files
- Execute ETL job
- Capture logs

**Usage:**
```bash
./scripts/etl_company_sales_spark_submit.sh
```

---

### Recommendation Script (`scripts/consumption_recommendation_spark_submit.sh`)

**Purpose:** Wrapper script for recommendation engine

**Responsibilities:**
- Execute recommendation generation
- Handle both company & competitor recommendations
- Manage output partitioning
- Log execution metrics

---

## ðŸ“ˆ Data Flow Summary

```
Raw CSV Files
    â†“
[ETL Scripts]
    â”œâ”€â†’ Bronze (Immutable Snapshot)
    â”œâ”€â†’ DQ Validation
    â”œâ”€â†’ Silver (Clean Data)
    â””â”€â†’ Quarantine (Failed Records)
    â†“
[Aggregation]
    â”œâ”€â†’ Company Sales Gold
    â”œâ”€â†’ Competitor Sales Gold
    â””â”€â†’ Seller Catalog Gold
    â†“
[Recommendation Engine]
    â”œâ”€â†’ Top-N Item Selection
    â”œâ”€â†’ Missing Item Identification
    â”œâ”€â†’ Revenue Estimation
    â†“
[Output]
    â”œâ”€â†’ company_seller_recommendation.csv
    â””â”€â†’ competitor_seller_recommendation.csv
```

---

## ðŸ” Module Dependencies

```
spark_session.py
    â”œâ”€â”€ pyspark
    â”œâ”€â”€ yaml
    â””â”€â”€ os

dq.py
    â”œâ”€â”€ pyspark.sql.functions
    â””â”€â”€ pyspark.sql.window

etl_company_sales.py
    â”œâ”€â”€ spark_session
    â”œâ”€â”€ dq
    â”œâ”€â”€ log
    â””â”€â”€ utils

consumption_recommendation.py
    â”œâ”€â”€ spark_session
    â”œâ”€â”€ dq
    â”œâ”€â”€ log
    â””â”€â”€ utils
```

---

## ðŸ“ File Naming Conventions

| Pattern | Purpose | Example |
|---------|---------|---------|
| `etl_*.py` | ETL pipeline scripts | `etl_company_sales.py` |
| `*_recommendation.py` | Recommendation logic | `consumption_recommendation.py` |
| `test_*.py` | Unit tests | `test_consumption_recommendations.py` |
| `validate_*.py` | Validation scripts | `validate_logic.py` |
| `*_spark_submit.sh` | Spark submission wrapper | `etl_company_sales_spark_submit.sh` |
| `*.yml` | Configuration files | `ecomm_prod.yml` |
| `*.conf` | Hudi/Spark config | `hudi-defaults.conf` |

---

## ðŸ” Security Considerations

### Credential Management
- AWS credentials via IAM role (preferred)
- Environment variables as fallback
- Never hardcode credentials

### Data Access
- S3 bucket policies
- IAM role-based access
- Encryption in transit & at rest

### Audit Trail
- Bronze layer preserves raw data
- Run timestamps for tracking
- Quarantine logs for DQ failures

---

## ðŸ“Š Key Metrics & Monitoring

### Execution Metrics
- Total records processed
- Valid records (Silver)
- Quarantine records
- Processing time
- Memory usage

### Data Quality Metrics
- Null value count
- Duplicate count
- Invalid date count
- Out-of-range numeric count

### Recommendation Metrics
- Recommendations per seller
- Average expected revenue
- Category distribution
- Coverage percentage

---

## ðŸ¤ Developer Workflow

### Adding a New ETL Script

1. Create `src/etl_new_source.py`
2. Import from `libs/spark_session`, `libs/dq`
3. Follow existing patterns (Bronze â†’ Silver â†’ Quarantine)
4. Add configuration to `configs/ecomm_prod.yml`
5. Create wrapper script in `scripts/`
6. Add tests in `tests/`

### Adding a New Utility Function

1. Add to appropriate `libs/*.py` file
2. Include docstring with examples
3. Add type hints
4. Update this documentation

### Running Tests

```bash
python tests/test_consumption_recommendations.py
python tests/validate_logic.py
```

---

**Last Updated:** November 2025  
**Version:** 1.0
